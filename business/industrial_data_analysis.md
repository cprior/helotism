---
layout: post
title:  "A blind spot in textbook service management"
date:   2016-03-22 20:00:00 +0100
---

The conceptual models of product Lifecycle Management, Application Lifecycle Management and IT Service Management are well-tested and applied in a wide range of enterprises. Nevertheless they almost obscure an important detail: The continous improvement process of a _solution_ is not covered in its entire complexity and its challenges and existing approaches therfore not enough understood.

![The spareparts cycle]({{ "/business/marketing/images/spareparts.svg" | prepend: site.baseurl }})  
*The spareparts cycle: Global Delivery Center, depots, technician's trunks, hubs, repair center.*

In this article a solution exists of both hardware and software which might well be both developed in-house or by suppliers or might be an integrated third-party component. Such a solution is comprises industrial control unit, microcontroller or even (embedded) PC unit(s). A solution such as a product in the sectors of mechanical engineering, automation or other (typical german) heavy industry is designed, manufactured, sold and in working state for many years, at minimum as long as the annual depreciation suggests. These solution are coverd by service contracts and repaired by the manufacturer's field service.

![The spareparts cycle]({{ "/business/marketing/images/complex-solution.svg" | prepend: site.baseurl }})  
*Representing a complex solution*

With (embedded) PC controls becoming ubiquitous and networked production no longer in the distant future the service organisation of a manufacturer of such a solution faces challenges formerly unknown: To not just support repair and maintenance incidents but also become proficient as a software support unit.

![The spareparts cycle]({{ "/business/marketing/images/PLM-ALM.svg" | prepend: site.baseurl }}){: .floatleft}  
*Commonly used Product Lifecycle and Application Lifecyle concepts*

What the previously monetioned concepts obscur: The service organisation must not only efficiently troubleshoot software incidents but also require changes in the solution that suit its needs and improve the serviceability of the solution.

With the traditional product lifecycle moving from a Q&A phase into a maintenance mode, and a standard application lifecycle iterating typically through several release cycles throughout the solution lifespan a huge technological gap needs to be filled within the service organisation.

Only those enterprises who are capable to process data generated by their solutions in the field will detect not only cost drivers for their service operation but also improve their solution in its entire complexity and start the next product development cycle on a higher  starting ground.


## An Example ##

As illustrated in the previous picture:

![The spareparts cycle]({{ "/business/marketing/images/spareparts.svg" | prepend: site.baseurl }})  
*New parts (green) coming in from the supplier, as-new parts (orange) from the repair center*

A field service call is resolved and the returned defective part reparied and stocked as-new. Within the typical material turnover timespan this part finds it way into another solution, perfectly valid as per part list and/or revision level of the material number.

It does contain though a previous firmware version. This is not immediately obvious as the new software of the solution is backward compatible. The old firmware does contain a bug though that is occasionally triggered.

The responsible service organisation must not only be able to cyclically perform a hardware- and software-inventory of all solutions under service contract, but also must be able to detect the unwanted configuration drift -- the latest when the next incident (triggered by the bug) is logged.

If the architecture of the solution does not grant access to these data snippets then the service organisation must be able to require the necessary changes in the product's maintenance phase and all future products.

To be capable to perform this role of an "ex post solution architect" either recurrent data transmissions from the solution into a data analysis workflow that is enriched by CRM and CMDB must be created, or at minimum during each field service dispatch such data must be collected.

## How to Deal with Data in an Industrial Service Context##

The following, general description of the existing problem space cannot articulate specific data needs of the service organisation. A first approximisation are these qualitative data aspects:

The solution, remotely accessible or not, contains a logging mechanism that stores data aligned with the business processes of the service organisation.

Depending on the transport layer for this data (if remote network acces or a plain USB stick of the field service engineer) the amount of stored log messages needs to be configurable in its quantity.

These service logs need to have their log level aligned with service requirements: In this context a "debug" level is for "bad boys" installations under closer monitoring. An "info" loglevel must contain all necessary data to troubleshoot a "first offender".

The data must be mapped not just to the customer but also to the location such as the customer's branch. This mapping must be part of every troubleshooting analysis because new pattern will emerge when correlating all data from one location. For example an error with a webservice call is put in a completely new light when another data snippet shows a network hickup at the same time.

![Data Mapping]({{ "/business/marketing/images/complex-solution_beyond-ITIL.svg" | prepend: site.baseurl }})  
*Correlate the data across multiple installations at the same customer*

Each of this data source either needs to be rock-solidly time-synchronized or the analysis tool must support time offsets per source and timespan.

Each such data snippet needs to be captured as a single data instance. This will typically exclude multi-line log formats if a traditional text logfile is used. Even machine parseable data still needs to carefully balance begin/end data or summarized data.

## The Canon of Support Data ##

Narrowing down the content of such data the following data snippets will typically be needed:

Version numbers of all (sic) components, either as firmware version, operating system version, package version of all major software components as well as revision levels of a part from the bill-of-materials, identified by the solution manufacturer's part number or the OEM part number.

The date and time is crucially important, not just for correlation but also to align with the incident. This observation might actually be the single most telling example of what this article describes: Software programmers typically rotate their logfiles by filesize, whereas an incident in the service organization bears a date and/or time. Each effort spent by the service organization to correlate indicent time with troubleshooting is a cost driver that will never be in scope of product development (time spent as in each incident as well as training of agents).

## Tools and Methods ##

The tools and methods to process such data already exist and can be aligned as following:

IT operations has a multitude of monitoring software. Whereas these tools might typically produce an alert in the context of this article rather a ticket needs to be created. The field service dispatcher then decides if it might be put on hold until the next service call in (or near) the location.

IT security has SIEM security information and event management software that is capable of correlating the occurrence of data snippets.

![Heterogenous Data Correlation]({{ "/business/marketing/images/YouTubeGoogleTechTalksRaffaelMartyLogManagementToday.png" | prepend: site.baseurl }})  
*The IT security experts have long figured out how to correlate across the infrastructure*

Traditional business warehouse software has the notion of ETL extract, transform, load additional data and therefore enriching it.

Log management has begun to treat logfiles as a stream of data and transports data across unreliable connections without overwhelming each piece of the infrastructure.

Powerful search engines provide a horizontally scaling architecture that powers not just textual searches but also dashboard vizualisations and visual analysis workflows.


<!--
correlate collect visualize enrich starschema extract transform load scale timestamp architecture collector generator archive align processe salert ticket CRM SIEM stream data transport enrich engine horizontal dashboard workflow
-->